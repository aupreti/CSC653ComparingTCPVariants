 \section{Methodology}\label{sec:methodology}
 \subsection{The Setup}
  \begin{figure}[!htbp]
  	\centering 
	\includegraphics[scale=0.4]{setup.png}
	\caption{We attach TCP agent with FTP as the application and UDP agent with a CBR flow.}
	\label{fig:setup}
\end{figure}
We use ns2 to create the topology in Figure \ref{fig:setup}. The setup consists of five different nodes, N1- N5. The link between each pair of nodes is a duplex link with a capacity of 10Mb.
\subsection{Calculation}
NS2 trace files were used to calculate drop rate, average throughput, average end-to-end latency and end-to-end latency over time for combinations of different TCP variants and different queuing algorithms. The trace files were parsed using a python script because of the ease of text parsing with python. We used a python library called the Matplotlib to generate the plots. We use a bash script to automate a series of experiments with varying TCP variant, CBR rate, CBR packet size and queuing algorithms. 
\begin{enumerate}
\item \textbf{Average Throughput:} To calculate the average throughput of a tcp stream, we divide the sum of packet sizes of all packets of the TCP stream that reached the destination by the total time of the tcp flow. 
\item \textbf{Latency:} To calculate the end-to-end latency over time, we recorded the time it took for each packet to get from the source to the destination. We took the average of these latencies to get the average latency. 
\item \textbf{Drop Rate:} The ns trace clearly identifies the dropped packets. To calculate the drop rate of a flow, we divided the total number of drops by the total number of packets in that flow.
\end{enumerate}
We did multiple runs of our simulations to verify that we see the same trends and reach similar conclusions in terms of throughput, latency and the drop rate. Whenever possible, we averaged our results over multiple runs. To verify that our results are statistically significant, this study can be extended by incorporating t tests . 
 \subsection{TCP Performance Under Congestion}
This experiment was designed to test the performance of different TCP variants Tahoe, Reno, NewReno and Vegas under various levels of congestion. TCP agent was attached to start at N1 and sink at N4. FTP application was attached to the TCP agent to simulate traffic that is typical of FTP. CBR flow was added to go from N2 to N3 to create congestion in the link between N2 and N3. We varied the CBR rate from 1 Mbps to 10 Mbps to create different levels of congestion. We had the liberty of choosing the packet size to maintain the CBR. To study the effects of varying CBR packet sizes, we also simulated varying packet sizes, 500B to 10,000B, for all four TCP variants. We did not change the default ns2 buffer size.
 \subsection{Fairness between TCP variants}
This experiment simulated two TCP flows at the same a time. The variant pairs that were simulated are (1) Tahoe/Tahoe (2) Reno/Reno (3) NewReno/Reno (4) Vegas/Vegas (5) NewReno/Vegas. Like before, to create varying levels of congestion, a CBR flow was added to originate at N2 and end at N3. The first TCP stream originated at added N1 and ended at N4. The second TCP stream originated at N5 and ended at N6. For each pair of the variants, we ran simulations for CBR ranging from 1 Mbps to 10 Mbps. The packet size was increased up from 500 for all the experiments. We started with 500B as the packet because it the size of an average IP packet size is around 600B. Accounting for the IP headers, we wanted a smaller packet size for UDP. 
We also started the two competing variants at different times and ran the simulations for a bit longer (15s) to check if a fair throughput could be achieved over time if one of the TCP streams was delayed.
 \subsection{Influence of Queuing}
In this experiment, we study the influence of queuing algorithms RED and DropTail on TCP Reno and SACK. The set up for this experiment is the same as experiment 1. We first start the TCP stream, wait till it has passes the slow start phase(at 4s) and then start the CBR flow. We conduct the experiments for two different buffer sizes, 100 and a 300. To compare the performance of RED and DropTail in the TCP variants and the CBR flow, we compare the differences in throughput over time, end-to-end latency over time and the drop rate between the queuing disciplines. 